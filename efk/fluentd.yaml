---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        imagePullPolicy: IfNotPresent
        env:
        - name: TZ
          valueFrom:
            configMapKeyRef:
              name: generic-parameters
              key: TZ
        - name:  FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.default"
        - name:  FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_SCHEME
          value: "http"
        - name: FLUENTD_SYSTEMD_CONF
          value: disable
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_INDEX_NAME
          value: default
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: default
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/conf.d
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config
        configMap:
          name: fluentd-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: kube-system
data:
  elasticsearch.conf.tmpl: |
    @log_level info
    include_tag_key true
    host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
    port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
    path "#{ENV['FLUENT_ELASTICSEARCH_PATH']}"
    scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
    ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
    ssl_version "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERSION'] || 'TLSv1_2'}"
    user "#{ENV['FLUENT_ELASTICSEARCH_USER'] || use_default}"
    password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD'] || use_default}"
    reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'false'}"
    reconnect_on_error "#{ENV['FLUENT_ELASTICSEARCH_RECONNECT_ON_ERROR'] || 'true'}"
    reload_on_failure "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE'] || 'true'}"
    log_es_400_reason "#{ENV['FLUENT_ELASTICSEARCH_LOG_ES_400_REASON'] || 'false'}"
    logstash_dateformat "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_DATEFORMAT'] || '%Y.%m.%d'}"
    logstash_format "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT'] || 'true'}"
    target_index_key "#{ENV['FLUENT_ELASTICSEARCH_TARGET_INDEX_KEY'] || use_nil}"
    type_name "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_TYPE_NAME'] || 'fluentd'}"
    include_timestamp "#{ENV['FLUENT_ELASTICSEARCH_INCLUDE_TIMESTAMP'] || 'false'}"
    template_name "#{ENV['FLUENT_ELASTICSEARCH_TEMPLATE_NAME'] || use_nil}"
    template_file "#{ENV['FLUENT_ELASTICSEARCH_TEMPLATE_FILE'] || use_nil}"
    template_overwrite "#{ENV['FLUENT_ELASTICSEARCH_TEMPLATE_OVERWRITE'] || use_default}"
    sniffer_class_name "#{ENV['FLUENT_SNIFFER_CLASS_NAME'] || 'Fluent::Plugin::ElasticsearchSimpleSniffer'}"
    request_timeout "#{ENV['FLUENT_ELASTICSEARCH_REQUEST_TIMEOUT'] || '5s'}"
    suppress_type_name "#{ENV['FLUENT_ELASTICSEARCH_SUPPRESS_TYPE_NAME'] || 'true'}"
    enable_ilm "#{ENV['FLUENT_ELASTICSEARCH_ENABLE_ILM'] || 'false'}"
    ilm_policy_id "#{ENV['FLUENT_ELASTICSEARCH_ILM_POLICY_ID'] || use_default}"
    ilm_policy "#{ENV['FLUENT_ELASTICSEARCH_ILM_POLICY'] || use_default}"
    ilm_policy_overwrite "#{ENV['FLUENT_ELASTICSEARCH_ILM_POLICY_OVERWRITE'] || 'false'}"
    <buffer>
      flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
      flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
      chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
      queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
      retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
      retry_forever true
    </buffer>

  microgateway.conf: |
    # Parse the logs as JSON
    <filter kubernetes.var.log.containers.microgateway**>
      @type parser
      key_name log
      reserve_data true
      remove_key_name_field false

      <parse>
        @type json
      </parse>
    </filter>

    <match kubernetes.var.log.containers.microgateway**>
      @type elasticsearch
      @id out_es_microgateway
      @include elasticsearch.conf.tmpl
      logstash_prefix airlock-waf-default
      index_name airlock-waf-default
    </match>

  iam.conf: |
    # Parse the logs as JSON
    <filter kubernetes.var.log.containers.iam**>
      @type parser
      key_name log
      reserve_data true
      remove_key_name_field false
    
      <parse>
        @type json
        time_key time
        time_type string
        time_format %Y-%m-%dT%H:%M:%S.%L%z
        keep_time_key true
      </parse>
    </filter>

    # <match kubernetes.var.log.containers.iam**>
    #   @type elasticsearch
    #   @id out_es_iam
    #   @include elasticsearch.conf.tmpl
    #   logstash_prefix airlock-iam-logs
    #   index_name airlock-iam-logs
    #   log_es_400_reason 'true'
    # </match>

  nginx-ingress.conf: |
    <source>
      @type tail
      path /var/log/containers/ingress-nginx-controller*.log
      pos_file /var/log/ingress-nginx-controller-containers.log.pos
      tag "#{ENV['FLUENT_CONTAINER_TAIL_TAG'] || 'kubernetes.*'}"
      read_from_head true
      <parse>
        @type json
      </parse>
    </source>
    
    # Parse the logs as JSON
    <filter kubernetes.var.log.containers.ingress-nginx-controller**>
      @type parser
      key_name log
      reserve_data true
      remove_key_name_field false
      
      # Enable `inject_key_prefix` to add a
      # prefix to the ingress keys
      inject_key_prefix ingress.
      <parse>
        @type json
      </parse>
    </filter>

    # Create the key 'req_id' to track the requests across applications.
    <filter kubernetes.var.log.containers.ingress-nginx-controller**>
      @type record_transformer
      enable_ruby
    
      <record>
        req_id ${JSON.parse(record['log'])['req_id']}
      </record>
      remove_keys "#{ENV['FLUENT_REMOVE_KEYS'] || 'log'}"
    </filter>

  echoserver.conf: |
    # Parse the logs as JSON
    <filter kubernetes.var.log.containers.echoserver**>
      @type parser
      key_name log
      reserve_data true
      remove_key_name_field false

      # Enable `inject_key_prefix` to add a
      # prefix to the echoserver keys
      inject_key_prefix echoserver.
      <parse>
        @type json
      </parse>
    </filter>

    # Create the key 'req_id' to track the requests across applications.
    <filter kubernetes.var.log.containers.echoserver**>
      @type record_transformer
      enable_ruby
    
      <record>
        req_id ${JSON.parse(record['log'])['http_req']['http_x_request_id']}
      </record>
      remove_keys "#{ENV['FLUENT_REMOVE_KEYS'] || 'log'}"
    </filter>